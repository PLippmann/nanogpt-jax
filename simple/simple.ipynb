{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple GPT in Jax"
      ],
      "metadata": {
        "id": "ilTFI4o5o1aN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's install and necessary dependencies that don't come with Colab."
      ],
      "metadata": {
        "id": "YwkTAaP5pKYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax optax tiktoken"
      ],
      "metadata": {
        "id": "GQx4Kls2o066"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, imports."
      ],
      "metadata": {
        "id": "FH02B4IIo0wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.lax as lax\n",
        "from jax import random, grad, jit, vmap\n",
        "import optax\n",
        "from typing import NamedTuple\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "niVQAledo2Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are our classes to store both the config of the model and params that we will pass around."
      ],
      "metadata": {
        "id": "u0wqNr8io3AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerConfig(NamedTuple):\n",
        "    \"\"\"Configuration for Transformer model.\n",
        "\n",
        "       vocab_size: Size of the vocabulary\n",
        "       block_size: Maximum sequence length\n",
        "       n_embd: Embedding dimension\n",
        "       n_head: Number of attention heads\n",
        "       n_layer: Number of transformer layers\n",
        "       dropout: Dropout probability\n",
        "       batch_size: Training batch size\n",
        "       learning_rate: Learning rate for optimization\n",
        "       max_iters: Maximum number of training iterations\n",
        "       eval_interval: Number of steps between evaluations\n",
        "       eval_iters: Number of evaluation iterations\n",
        "       init_scale: Scale for weight initialization\n",
        "    \"\"\"\n",
        "    vocab_size: int = 50304  # GPT-2 vocabulary size\n",
        "    block_size: int = 256\n",
        "    n_embd: int = 384\n",
        "    n_head: int = 6\n",
        "    n_layer: int = 6\n",
        "    dropout: float = 0.2\n",
        "    batch_size: int = 32\n",
        "    learning_rate: float = 3e-4\n",
        "    max_iters: int = 1500\n",
        "    eval_interval: int = 500\n",
        "    eval_iters: int = 200\n",
        "    init_scale: float = 0.02\n",
        "\n",
        "    def __post_init__(self):\n",
        "        assert self.n_embd % self.n_head == 0, \"n_embd must be divisible by n_head\"\n",
        "        assert self.dropout >= 0 and self.dropout <= 1, \"dropout must be between 0 and 1 inclusive\"\n",
        "\n",
        "class TransformerParams(NamedTuple):\n",
        "    token_embedding: jnp.ndarray\n",
        "    position_embedding: jnp.ndarray\n",
        "    layer_norms: list\n",
        "    attention_weights: list\n",
        "    attention_projections: list\n",
        "    mlp_weights: list\n",
        "    final_layer_norm: jnp.ndarray\n",
        "    lm_head: jnp.ndarray"
      ],
      "metadata": {
        "id": "tssio9gdo9hc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the main functions we will use to get the data through the transformer, incuding initialization, the attention mechanism, and the forward pass. All implemented using @JIT for better performance."
      ],
      "metadata": {
        "id": "B8i_DA9xo91f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k6wm0a_fPzej"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def get_sequence(data, start_idx):\n",
        "    \"\"\"Extract a sequence of tokens and the corresponding targets from the data.\"\"\"\n",
        "    x_seq = lax.dynamic_slice(data, (start_idx,), (config.block_size,))\n",
        "    y_seq = lax.dynamic_slice(data, (start_idx + 1,), (config.block_size,))\n",
        "\n",
        "    return x_seq, y_seq\n",
        "\n",
        "@jit\n",
        "def get_batch(data, rng_key):\n",
        "    \"\"\"Generate a batch of data for training or validation.\"\"\"\n",
        "    data_size = data.shape[0]\n",
        "    max_start_idx = data_size - config.block_size\n",
        "    ix = jax.random.randint(rng_key, (config.batch_size,), 0, max_start_idx)\n",
        "    x, y = jax.vmap(lambda idx: get_sequence(data, idx))(ix)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def init_params(rng):\n",
        "    \"\"\"Initialize model parameters.\"\"\"\n",
        "    rngs = jax.random.split(rng, 8)\n",
        "\n",
        "    token_embedding = jax.random.normal(rngs[0], (config.vocab_size, config.n_embd)) * 0.02\n",
        "    position_embedding = jax.random.normal(rngs[1], (config.block_size, config.n_embd)) * 0.02\n",
        "\n",
        "    layer_norms, attention_weights, attention_projections, mlp_weights = [], [], [], []\n",
        "    for _ in range(config.n_layer):\n",
        "        # Initialize layer norm parameters with scale and bias\n",
        "        layer_norms.append(jnp.ones((config.n_embd,)))\n",
        "        attention_weights.append(jax.random.normal(rngs[2], (config.n_embd, 3 * config.n_embd)) * 0.02)\n",
        "        attention_projections.append(jax.random.normal(rngs[3], (config.n_embd, config.n_embd)) * 0.02)\n",
        "        mlp_weights.append({\n",
        "            'c_fc': jax.random.normal(rngs[4], (config.n_embd, 4 * config.n_embd)) * 0.02,\n",
        "            'c_proj': jax.random.normal(rngs[5], (4 * config.n_embd, config.n_embd)) * 0.02\n",
        "        })\n",
        "\n",
        "    # Initialize final layer norm with scale and bias\n",
        "    final_layer_norm = jnp.ones((config.n_embd,))\n",
        "    lm_head = jax.random.normal(rngs[6], (config.n_embd, config.vocab_size)) * 0.02\n",
        "\n",
        "    return TransformerParams(\n",
        "        token_embedding=token_embedding,\n",
        "        position_embedding=position_embedding,\n",
        "        layer_norms=layer_norms,\n",
        "        attention_weights=attention_weights,\n",
        "        attention_projections=attention_projections,\n",
        "        mlp_weights=mlp_weights,\n",
        "        final_layer_norm=final_layer_norm,\n",
        "        lm_head=lm_head\n",
        "    )\n",
        "\n",
        "@jit\n",
        "def layer_norm(x, weight):\n",
        "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
        "    variance = jnp.var(x, axis=-1, keepdims=True)\n",
        "\n",
        "    return weight * (x - mean) / jnp.sqrt(variance + 1e-5)\n",
        "\n",
        "@jit\n",
        "def attention(q, k, v, mask=None):\n",
        "    \"\"\"Compute attention scores and perform weighted aggregation.\"\"\"\n",
        "    head_dim = q.shape[-1]\n",
        "    attn = jnp.einsum('bhid,bhjd->bhij', q, k) / jnp.sqrt(head_dim)\n",
        "\n",
        "    if mask is not None:\n",
        "        attn = jnp.where(mask == 0, float('-inf'), attn)\n",
        "\n",
        "    attn = jax.nn.softmax(attn, axis=-1)\n",
        "    out = jnp.einsum('bhij,bhjd->bhid', attn, v)\n",
        "\n",
        "    return out\n",
        "\n",
        "def forward(params, x, key, training=False):\n",
        "    \"\"\"Forward pass through the transformer model.\"\"\"\n",
        "    b, t = x.shape\n",
        "    head_size = config.n_embd // config.n_head\n",
        "    token_emb = params.token_embedding[x]\n",
        "    pos = jnp.arange(t)\n",
        "    pos_emb = params.position_embedding[pos]\n",
        "    x = token_emb + pos_emb\n",
        "    mask = jnp.tril(jnp.ones((t, t)))\n",
        "\n",
        "    for i in range(config.n_layer):\n",
        "        # Layer normalization with scale and bias\n",
        "        ln1 = layer_norm(x, params.layer_norms[i])\n",
        "        qkv = jnp.dot(ln1, params.attention_weights[i])\n",
        "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
        "        q = q.reshape(b, t, config.n_head, head_size).transpose(0, 2, 1, 3)\n",
        "        k = k.reshape(b, t, config.n_head, head_size).transpose(0, 2, 1, 3)\n",
        "        v = v.reshape(b, t, config.n_head, head_size).transpose(0, 2, 1, 3)\n",
        "\n",
        "        att = attention(q, k, v, mask)\n",
        "        att = att.transpose(0, 2, 1, 3).reshape(b, t, config.n_embd)\n",
        "        att = jnp.dot(att, params.attention_projections[i])\n",
        "        x = x + att\n",
        "\n",
        "        ln2 = layer_norm(x, params.layer_norms[i])\n",
        "        mlp = jnp.dot(ln2, params.mlp_weights[i]['c_fc'])\n",
        "        mlp = jax.nn.gelu(mlp)\n",
        "        mlp = jnp.dot(mlp, params.mlp_weights[i]['c_proj'])\n",
        "        x = x + mlp\n",
        "\n",
        "        if training:\n",
        "            dropout_key, key = jax.random.split(key)\n",
        "            x = jnp.where(jax.random.uniform(dropout_key, x.shape) > config.dropout, x, 0)\n",
        "\n",
        "    # Final layer normalization\n",
        "    x = layer_norm(x, params.final_layer_norm)\n",
        "    logits = jnp.dot(x, params.lm_head)\n",
        "\n",
        "    return logits\n",
        "\n",
        "@jit\n",
        "def loss_fn(params, batch, key):\n",
        "    \"\"\"Compute loss over a batch of data.\"\"\"\n",
        "    x, y = batch\n",
        "    logits = forward(params, x, key)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits.reshape(-1, logits.shape[-1]),\n",
        "        y.reshape(-1)\n",
        "    )\n",
        "\n",
        "    return jnp.mean(loss)\n",
        "\n",
        "@jit\n",
        "def train_step(params, opt_state, batch, key):\n",
        "    \"\"\"Perform a single training step.\"\"\"\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, batch, key)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return params, opt_state, loss\n",
        "\n",
        "@jit\n",
        "def generate_step(params, x, key):\n",
        "    \"\"\"Single forward step with temperature-adjusted sampling.\"\"\"\n",
        "    logits = forward(params, x[None, :], key, training=False)[0]\n",
        "    next_token_logits = logits[-1, :]  # Get logits for the last position\n",
        "    # Apply temperature sampling\n",
        "    temperature = 0.8  # Adjust this value to control randomness (lower = more focused)\n",
        "    next_token_logits = next_token_logits / temperature\n",
        "    # Sample from the distribution\n",
        "    next_token = jax.random.categorical(key, next_token_logits)\n",
        "\n",
        "    return next_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function that actually takes in the prompt and outputs our generated tokens."
      ],
      "metadata": {
        "id": "agbdv772qNA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(params, prompt, max_new_tokens=100, temperature=0.8):\n",
        "    \"\"\"Generate text from a prompt.\"\"\"\n",
        "    # Encode the prompt\n",
        "    input_ids = jnp.array(enc.encode(prompt))\n",
        "\n",
        "    # Initialize generation\n",
        "    generated = list(input_ids)\n",
        "    key = jax.random.PRNGKey(0)\n",
        "\n",
        "    # Generate tokens\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Get the window of tokens that fits our model's context size\n",
        "        x = jnp.array(generated[-config.block_size:] if len(generated) > config.block_size else generated)\n",
        "\n",
        "        # Get next token\n",
        "        key, subkey = jax.random.split(key)\n",
        "        next_token = generate_step(params, x, subkey)\n",
        "\n",
        "        # Append to generated sequence\n",
        "        generated.append(next_token)\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    return enc.decode(generated)"
      ],
      "metadata": {
        "id": "KeAAblEpqMeM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our main loop used for training the model and outputing some test tokens. We train for 1500 iterations, which is already enough for the model to overfit. Finally, we output 200 token's worth of new text based on our single word prompt."
      ],
      "metadata": {
        "id": "bvraVdHGpEP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize configuration\n",
        "    config = TransformerConfig()\n",
        "\n",
        "    # RNG setup\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "    rng, init_rng = jax.random.split(rng)\n",
        "\n",
        "    # Download data (wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
        "    with open('./data/tinysp/input.txt', 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        print(text[:20])\n",
        "\n",
        "    # Tokenizer setup\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
        "\n",
        "    # Train and test splits\n",
        "    data = jnp.array(enc.encode(text))\n",
        "    train_data = jnp.array(data[:int(0.9 * len(data))])\n",
        "    val_data = jnp.array(data[int(0.9 * len(data)):])\n",
        "    print(data.shape, train_data.shape, val_data.shape, data[:10])\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    params = init_params(init_rng)\n",
        "    optimizer = optax.adam(config.learning_rate)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    # Main training loop\n",
        "    for iter in range(config.max_iters):\n",
        "        rng, split_key = jax.random.split(rng)\n",
        "        batch = get_batch(train_data, split_key)\n",
        "        params, opt_state, loss = train_step(params, opt_state, batch, split_key)\n",
        "\n",
        "        if iter % config.eval_interval == 0:\n",
        "            losses = []\n",
        "            for _ in range(config.eval_iters):\n",
        "                rng, split_key = jax.random.split(rng)\n",
        "                batch = get_batch(val_data, split_key)\n",
        "                losses.append(loss_fn(params, batch, split_key))\n",
        "            print(f\"step {iter}: train loss {loss:.4f}, val loss {jnp.mean(jnp.array(losses)):.4f}\")\n",
        "\n",
        "    # Example usage:\n",
        "    prompt = \"Behold,\"\n",
        "    print(\"\\nGenerating text from prompt:\", prompt)\n",
        "    print(\"-\" * 40)\n",
        "    generated_text = generate_text(params, prompt, max_new_tokens=200)\n",
        "    print(generated_text)"
      ],
      "metadata": {
        "id": "gx-5NVkHdxME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2faafb95-4876-468d-e55d-09748462580a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Befor\n",
            "(338025,) (304222,) (33803,) [ 5962 22307    25   198  8421   356  5120   597  2252    11]\n",
            "step 0: train loss 10.8821, val loss 10.3446\n",
            "step 500: train loss 3.6792, val loss 4.7644\n",
            "step 1000: train loss 1.8331, val loss 5.6993\n",
            "\n",
            "Generating text from prompt: Behold,\n",
            "----------------------------------------\n",
            "Behold, that their designiness,\n",
            "Six of his faults, and there is grown in time\n",
            "To diswealth,\n",
            "Will he free contempt shall deserve to put up,\n",
            "Save him from o'er a king?\n",
            "\n",
            "LEONTES:\n",
            "That you, if he do so, we partly think,Is it good, even so. My god\n",
            "Is't my young vilely\n",
            "That he my daughter? had rather\n",
            "Than a necessity in him, my wife is the king\n",
            "And this dead guest: all I am the queen,\n",
            "My wife is wind and heart good queens.\n",
            "\n",
            "Nurse:\n",
            "Ay, but I'll go at any man.\n",
            "\n",
            "PAULINA:\n",
            "A sour lord,\n",
            "It is honesty and call it stay the last\n",
            "Shame to my kindred's vault.\n",
            "\n",
            "LEONTES:\n",
            "O, not, you need the wife.\n",
            "You will not think so? his precious deed twice me?\n",
            "You are laid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UMbKkBhfrscq"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}